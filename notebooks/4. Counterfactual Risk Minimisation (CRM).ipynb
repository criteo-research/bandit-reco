{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0QUSEkBAWGYe"
   },
   "source": [
    "# Counterfactual Risk Minimisation (CRM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "colab_type": "code",
    "id": "5dPUazDAWGYh",
    "outputId": "ca0f3d64-e840-4c5b-ba7a-7337989a31d6"
   },
   "outputs": [],
   "source": [
    "!pip install recogym torch\n",
    "!pip install git+git://github.com/criteo-research/bandit-reco.git#subdirectory=notebooks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ulJfFdaQWGYp"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import gym \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from recogym.envs.session import OrganicSessions\n",
    "from recogym import env_1_args, Configuration\n",
    "from recogym import DefaultContext\n",
    "from recogym.agents import Agent\n",
    "from recogym.envs.observation import Observation\n",
    "from recogym import verify_agents\n",
    "from recogym.evaluate_agent import plot_verify_agents\n",
    "from recogym.agents import FeatureProvider, OrganicUserEventCounterAgent, organic_user_count_args\n",
    "\n",
    "from util import FullBatchLBFGS\n",
    "\n",
    "# Set style for pretty plots\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "P = 10  # Number of Products\n",
    "U = 2000 # Number of Users\n",
    "\n",
    "# You can overwrite environment arguments here:\n",
    "env_1_args['random_seed'] = 42\n",
    "env_1_args['num_products'] = P\n",
    "env_1_args['number_of_flips'] = P//2\n",
    "env_1_args['sigma_mu_organic'] = 3\n",
    "env_1_args['sigma_omega'] = 0.1\n",
    "\n",
    "# Initialize the gym for the first time by calling .make() and .init_gym()\n",
    "env = gym.make('reco-gym-v1')\n",
    "env.init_gym(env_1_args)\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wFE_wsZrk1Md"
   },
   "source": [
    "## Data generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "42T1oNhgj5RF"
   },
   "source": [
    "We generate data with a popularity agent plus some randomness for exploration (`epsilon`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "CGUHO1MuWGYr",
    "outputId": "efa103ae-626e-4cff-fcc0-76904053bcc7"
   },
   "outputs": [],
   "source": [
    "# Generate RecoGym logs for U users\n",
    "logger = OrganicUserEventCounterAgent(Configuration({**organic_user_count_args,\n",
    "                                                     'select_randomly': True,\n",
    "                                                     'random_seed': 1221, # 1220\n",
    "                                                     'epsilon': 0.001}))\n",
    "reco_log = env.generate_logs(U, agent = logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oeHO1popj1Cd"
   },
   "source": [
    "Let's have a quick look at the logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "pz9y0SwJWGYu",
    "outputId": "db023e70-e80e-44e5-dda8-2b9cadc84392"
   },
   "outputs": [],
   "source": [
    "reco_log.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3DRJls4LlDJu"
   },
   "source": [
    "We will use the popularity agent as the baseline. It basically predict the organic product that has been seen the most so far by the user. It is a simple but actually strong baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Trdgc94glByy"
   },
   "outputs": [],
   "source": [
    "organic_counter_agent = OrganicUserEventCounterAgent(\n",
    "    Configuration({**organic_user_count_args, 'select_randomly': False, 'exploit_explore': False, 'epsilon': 0.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "colab_type": "code",
    "id": "PckUhxfVlOb3",
    "outputId": "81965699-f401-40a3-b419-83451eddf364"
   },
   "outputs": [],
   "source": [
    "N_USERS_AB_TEST = 5000\n",
    "result_AB = verify_agents(env, N_USERS_AB_TEST, {'User-pop': organic_counter_agent})\n",
    "plot_verify_agents(result_AB)\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "puyqtiMqWGYw"
   },
   "source": [
    "## Contextual Bandits\n",
    "\n",
    "### Data set transformation\n",
    "\n",
    "In order to feed logs to a model, we need to transform them such that we have a dataset consisting in sequences of user context $x$, taken action $a$, reward $c$ and propensity scores $p$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PqXrnNwLqwd_"
   },
   "outputs": [],
   "source": [
    "class ProductCountFeatureProvider(FeatureProvider):\n",
    "    \"\"\"This feature provider creates a user state based on viewed product count.\n",
    "    Namely, the feature vector of shape (n_products, ) contains for each product how many times the\n",
    "    user has viewed them organically.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(ProductCountFeatureProvider, self).__init__(config)\n",
    "        self.feature_data = np.zeros((self.config.num_products)).astype(int)\n",
    "\n",
    "    def observe(self, observation):\n",
    "        for session in observation.sessions():\n",
    "            self.feature_data[int(session['v'])] += 1\n",
    "\n",
    "    def features(self, observation):\n",
    "        return self.feature_data.copy()\n",
    "\n",
    "    def reset(self):\n",
    "        self.feature_data[:] = 0\n",
    "\n",
    "\n",
    "def build_rectangular_data(logs, feature_provider):\n",
    "    \"\"\"Create a rectangular feature set from the logged data.\n",
    "    For each taken action, we compute the state in which the user was when the action was taken\n",
    "    \"\"\"\n",
    "    user_states, actions, rewards, proba_actions = [], [], [], []\n",
    "    \n",
    "    current_user = None\n",
    "    for _, row in logs.iterrows():\n",
    "        if current_user != row['u']:\n",
    "            # Use has changed: start a new session and reset user state\n",
    "            current_user = row['u']\n",
    "            sessions = OrganicSessions()\n",
    "            feature_provider.reset()\n",
    "        \n",
    "        context = DefaultContext(row['u'], row['t'])\n",
    "        \n",
    "        if row['z'] == 'organic':\n",
    "            sessions.next(context, row['v'])\n",
    "            \n",
    "        else:\n",
    "            # For each bandit event, generate one observation for the user state, the taken action\n",
    "            # the obtained reward and the used probabilities\n",
    "            feature_provider.observe(Observation(context, sessions))\n",
    "            user_states += [feature_provider.features(None)] \n",
    "            actions += [row['a']]\n",
    "            rewards += [row['c']]\n",
    "            proba_actions += [row['ps']] \n",
    "            \n",
    "            # Start a new organic session\n",
    "            sessions = OrganicSessions()\n",
    "    \n",
    "    return np.array(user_states), np.array(actions).astype(int), np.array(rewards), np.array(proba_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mSTZDLmumcxn"
   },
   "source": [
    "Let's build and dissect this so-called rectangular dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G5XkYyU8o_qS"
   },
   "outputs": [],
   "source": [
    "# You can now see data that will be provided to our agents based on logistic regressions\n",
    "count_product_views_feature_provider = ProductCountFeatureProvider(config=Configuration(env_1_args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d7u0bEWymk4y"
   },
   "outputs": [],
   "source": [
    "rectangular_logs = build_rectangular_data(reco_log, count_product_views_feature_provider)\n",
    "user_states, actions, rewards, proba_actions = rectangular_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "BDjWBg4Jq3pr",
    "outputId": "4045b37c-0f62-4846-bbb1-22fb9a5de1a4"
   },
   "outputs": [],
   "source": [
    "preview_start, preview_size = 300, 3\n",
    "\n",
    "print('user product views count at action time')\n",
    "print(user_states[preview_start:preview_start + preview_size])\n",
    "print('taken actions', actions[preview_start:preview_start + preview_size])\n",
    "print('obtained rewards', rewards[preview_start:preview_start + preview_size])\n",
    "print('probablities of the taken actions', proba_actions[preview_start:preview_start + preview_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7vBeh24FlntC"
   },
   "source": [
    "### Vanilla contextual bandit\n",
    "\n",
    "Classical value-based methods aim to learn the probability that a given action will lead to a positive reward, that is:\n",
    "\n",
    "$$p(c = 1|\\mathbf{x},a)$$\n",
    "\n",
    "In what follows, we will implement a different approach: a policy learning method.\n",
    "Policy-learning methods do not explicitly model the probability of a positive reward, but aim to directly model the action that should be taken, given a context:\n",
    "\n",
    "$$p(a|\\mathbf{x})$$\n",
    "\n",
    "Classical contextual bandits achieve this by optimising the expectation of the reward under the new (learned) policy $\\pi_\\theta$, given a logged dataset under policy $\\pi_0$.\n",
    "For a given dataset $\\mathcal{D}$ consisting of $N$ tuples $(\\mathbf{x},a,p,c)$, the objective can be written as the following:\n",
    "\n",
    "$$\\theta^{*} = \\text{argmax}_{\\theta} \\sum_{i=1}^{N}c_i\\frac{\\pi_\\theta(a_i|\\mathbf{x}_i)}{\\pi_0(a_i|\\mathbf{x}_i)}$$\n",
    "\n",
    "This objective can be straightforwardly optimised using your favourite package that provides auto-differentiation functionality.\n",
    "In our example, we will use PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vrTg8M8ct8K8"
   },
   "outputs": [],
   "source": [
    "class MultinomialLogisticRegressionModel(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        torch.nn.Module.__init__(self)\n",
    "        # Generate weights - initialise randomly\n",
    "        self.weight = torch.nn.Parameter(torch.Tensor(output_dim, input_dim))\n",
    "        # torch.nn.init.kaiming_uniform_(self.weight, a = np.sqrt(5))\n",
    "        torch.nn.init.zeros_(self.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        inner_product = F.linear(x, self.weight)\n",
    "        return F.softmax(inner_product, dim = 1)\n",
    "\n",
    "class VanillaContextualBandit(Agent):\n",
    "    def __init__(self, config, U = U, P = P, max_epoch=30):\n",
    "        Agent.__init__(self, config)\n",
    "        self.model = MultinomialLogisticRegressionModel(P, P)\n",
    "        self.loss_history = []\n",
    "        self.user_state = np.zeros(P)\n",
    "        self.U = U\n",
    "        self.P = P\n",
    "        self.max_epoch = max_epoch\n",
    "\n",
    "    def loss(self, X, a, proba_logged_actions, c):\n",
    "        # Compute action predictions for clicks\n",
    "        predicted_proba_for_all_actions = self.model(X)\n",
    "        \n",
    "        # Only keep probabilities for the actions that were taken\n",
    "        predicted_proba = torch.gather(predicted_proba_for_all_actions, 1, a.unsqueeze(1)).reshape(-1)\n",
    "        \n",
    "        # expectation of the rewards under the new policy\n",
    "        rewards = predicted_proba / proba_logged_actions\n",
    "        \n",
    "        # code here\n",
    "        # We can cap the weights here (or equivalently the rewards in our case)\n",
    "        # to some value of our choosing\n",
    "\n",
    "        # Since pytorch is meant to perform convex optimization, we rather\n",
    "        # output a loss that we will want to minimize\n",
    "        loss = - rewards\n",
    "        return loss.mean()\n",
    "\n",
    "    def train(self, rectangular_logs):\n",
    "        \"\"\"Train the contextual bandit based on an offline log such that it \n",
    "        learns to minimize its loss function\n",
    "        \"\"\"\n",
    "        user_states, actions, rewards, proba_actions = rectangular_logs\n",
    "        X = user_states\n",
    "        a = actions\n",
    "        p = proba_actions\n",
    "        c = rewards\n",
    "        \n",
    "        # Put into PyTorch variables - drop unclicked samples\n",
    "        X = Variable(torch.Tensor(X[c != 0]))\n",
    "        a = Variable(torch.LongTensor(a[c != 0]))\n",
    "        w = torch.Tensor(p[c != 0])\n",
    "        \n",
    "        def closure():\n",
    "            # Reset gradients\n",
    "            optimiser.zero_grad()\n",
    "\n",
    "            return self.loss(X, a, w, c)\n",
    "        \n",
    "        # Set up optimiser\n",
    "        optimiser = FullBatchLBFGS(self.model.parameters())\n",
    "\n",
    "        # Initial loss\n",
    "        self.loss_history.append(closure())\n",
    "        max_epoch = self.max_epoch\n",
    "        for epoch in range(max_epoch):\n",
    "            # Optimisation step\n",
    "            obj, _, _, _, _, _, _, _ = optimiser.step({'closure': closure,\n",
    "                                                       'current_loss': self.loss_history[-1],\n",
    "                                                       'max_ls': 20})\n",
    "            self.loss_history.append(obj)\n",
    "        \n",
    "        return\n",
    "\n",
    "    def plot_loss_history(self):\n",
    "        ''' Plot the training loss over epochs '''\n",
    "        _,_ = plt.subplots()\n",
    "        plt.plot(range(len(self.loss_history)),self.loss_history)\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('loss')\n",
    "        plt.show()\n",
    "        \n",
    "    def observe(self, observation):\n",
    "        ''' Observe new organic views and capture them in the user state '''\n",
    "        for session in observation.sessions():\n",
    "            self.user_state[int(session['v'])] += 1\n",
    "\n",
    "    def act(self, observation, reward, done):\n",
    "        ''' Pick an action, based on the current observation and the history '''\n",
    "        # Observe\n",
    "        self.observe(observation)\n",
    "\n",
    "        # Act\n",
    "        p_a = self.model(torch.Tensor([self.user_state])).detach().numpy().ravel()\n",
    "        action = np.argmax(p_a)\n",
    "        prob = np.zeros_like(p_a)\n",
    "        prob[action] = 1.0\n",
    "\n",
    "        return {\n",
    "            **super().act(observation, reward, done),\n",
    "            **{\n",
    "                'a': action,\n",
    "                'ps': 1.0,\n",
    "                'ps-a': prob,\n",
    "            },\n",
    "        }\n",
    "\n",
    "    def reset(self):\n",
    "        ''' Reset the user state '''\n",
    "        self.user_state = np.zeros(self.P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "colab_type": "code",
    "id": "7-sqFQ5zWGYz",
    "outputId": "25db8e78-71f0-4757-f1d6-4c395904a3d3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vanilla_cb = VanillaContextualBandit({}, max_epoch=100)\n",
    "vanilla_cb.train(rectangular_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla_cb.plot_loss_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 362
    },
    "colab_type": "code",
    "id": "5h-81oqEZLaA",
    "outputId": "69f202fa-741d-4a41-e101-ad8552732c55"
   },
   "outputs": [],
   "source": [
    "result_AB = pd.concat([result_AB, verify_agents(env, N_USERS_AB_TEST, {'Contextual Bandit': vanilla_cb})])\n",
    "plot_verify_agents(result_AB)\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UMuDCT71kEF7"
   },
   "source": [
    "It appears that some clicked events were actually sampled with a small probability ($\\pi_0 \\ll 1$). This might lead to overfitting for the contextual bandit and bad online performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "TIW2y9ORnHLO",
    "outputId": "5d8a2cb6-f6f9-4680-c130-0f17dd56ad92"
   },
   "outputs": [],
   "source": [
    "(reco_log[reco_log['c'] == 1]).sort_values('ps').head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VR0r1kvpWGY1"
   },
   "source": [
    "## Log Contextual Bandit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qHTJMOW_HuVs"
   },
   "outputs": [],
   "source": [
    "class LogContextualBandit(VanillaContextualBandit):\n",
    "\n",
    "    def loss(self, X, a, proba_logged_actions, c):\n",
    "        # Compute action predictions for clicks\n",
    "        predicted_proba_for_all_actions = self.model(X)\n",
    "        \n",
    "        # Only keep probabilities for the actions that were taken\n",
    "        predicted_proba = torch.gather(predicted_proba_for_all_actions, 1, a.unsqueeze(1)).reshape(-1)\n",
    "        \n",
    "        # expectation of the reward under the new policy\n",
    "        # code here\n",
    "        # reward = \n",
    "        \n",
    "        loss = -reward\n",
    "\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "colab_type": "code",
    "id": "cTSdaQTIWGY2",
    "outputId": "98f31ab4-21d7-4c89-e65a-e6ef3e681558",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "log_cb = LogContextualBandit({}, max_epoch=200)\n",
    "log_cb.train(rectangular_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_cb.plot_loss_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "colab_type": "code",
    "id": "eirfds7GWGY3",
    "outputId": "da4b4787-95ab-44e5-ea64-ef0bf35cbf65"
   },
   "outputs": [],
   "source": [
    "result_AB = pd.concat([result_AB, verify_agents(env, N_USERS_AB_TEST, {'Log Contextual Bandit': log_cb})])\n",
    "plot_verify_agents(result_AB)\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h7dIHhB1WGY8"
   },
   "source": [
    "## POEM\n",
    "\n",
    "The classical IPS estimator has its disadvantages, however.\n",
    "In its current form, the variance of the estimate can grow to be of significant size.\n",
    "To mitigate this, Swaminathan and Joachims propose to include a sample variance penalisation term to the objective, effectively ensuring that the learned model does not stray too far from the logging policy.\n",
    "\n",
    "This is the Counterfactual Risk Minimisation (CRM) objective, and the learning method optimising it directly is called POEM:\n",
    "$$\\theta^{*} = \\text{argmax}_{\\theta} \\sum_{i=1}^{N}c_i\\frac{\\pi_\\theta(a_i|\\mathbf{x}_i)}{\\pi_0(a_i|\\mathbf{x}_i)} - \\lambda \\sqrt{\\frac{\\widehat{Var}_\\theta}{N}} $$\n",
    "\n",
    "\n",
    "We model $\\pi_\\theta$ as linear:\n",
    "\n",
    "$$\\pi_\\theta(a|\\mathbf{x}) = \\text{softmax}(\\mathbf{x}^{\\intercal}\\theta)_a$$\n",
    "\n",
    "\n",
    "Swaminathan, Adith, and Thorsten Joachims. \"Batch learning from logged bandit feedback through counterfactual risk minimization.\" Journal of Machine Learning Research 16.1 (2015): 1731-1755."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "84mS8hNKIMYW"
   },
   "outputs": [],
   "source": [
    "class PoemContextualBandit(VanillaContextualBandit):\n",
    "    def __init__(self, config, U = U, P = P, max_epoch=30, variance_penalization_factor=0.):\n",
    "        VanillaContextualBandit.__init__(self, config, U=U, P=P, max_epoch=max_epoch)\n",
    "        self.variance_penalization_factor = variance_penalization_factor\n",
    "\n",
    "    def loss(self, X, a, proba_logged_actions, c):\n",
    "        # Compute action predictions for clicks\n",
    "        predicted_proba_for_all_actions = self.model(X)\n",
    "        \n",
    "        # Only keep probabilities for the actions that were taken\n",
    "        predicted_proba = torch.gather(predicted_proba_for_all_actions, 1, a.unsqueeze(1)).reshape(-1)\n",
    "        \n",
    "        # expectation of the loss under the new policy\n",
    "        # code here\n",
    "        # reward = \n",
    "        # Note: have a look at torch.sqrt and torch.var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "umlUlCGLFjxb",
    "outputId": "b38ea2bc-f6ca-4cac-af2a-24956182d3dd"
   },
   "outputs": [],
   "source": [
    "poem = PoemContextualBandit({}, variance_penalization_factor=0.4, max_epoch=100)\n",
    "poem.train(rectangular_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poem.plot_loss_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "colab_type": "code",
    "id": "fj2W8CC7GCJn",
    "outputId": "d971fd45-3871-47be-826d-a646ac284da4"
   },
   "outputs": [],
   "source": [
    "result_AB = pd.concat([result_AB, verify_agents(env, N_USERS_AB_TEST, {f'POEM': poem})])\n",
    "plot_verify_agents(result_AB)\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "4. Counterfactual Risk Minimisation (CRM).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
